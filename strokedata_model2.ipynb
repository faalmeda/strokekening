{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/faalmeda/strokekening/blob/main/strokedata_model2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97gcMRmnFN_F",
        "outputId": "80760adb-4243-46e2-ab8c-4653ea2375db"
      },
      "id": "97gcMRmnFN_F",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "from PIL import Image\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "PoliRJqojMwZ"
      },
      "id": "PoliRJqojMwZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.path.exists('/content/drive/MyDrive/Data/dataset_augmented4/train/normal/'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anxDq0K2RU7k",
        "outputId": "66fb5fdf-bc83-47f7-d01d-c07dcd8909fa"
      },
      "id": "anxDq0K2RU7k",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "f1ca04b6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1ca04b6",
        "outputId": "1a4eed3b-994d-463e-f5a1-e281617cd2e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 20793\n",
            "Validation samples: 462\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# ---Transformations---\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# ---Loader Dataset Class---\n",
        "class StrokeDataset(Dataset):\n",
        "    def __init__(self, root, transform=None):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "\n",
        "        for label, category in enumerate(['normal', 'stroke']):\n",
        "            category_path = os.path.join(root, category)\n",
        "            for img in os.listdir(category_path):\n",
        "                self.images.append(os.path.join(category, img))\n",
        "                self.labels.append(label) # 0 for normal, 1 for stroke\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root, self.images[idx])\n",
        "        label = self.labels[idx]\n",
        "        img = Image.open(img_path).convert('L')\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "\n",
        "# ---Local Directories used---\n",
        "train_dir = '/content/drive/MyDrive/Data/dataset_augmented4/train/'\n",
        "val_dir = '/content/drive/MyDrive/Data/dataset_split3/val/'\n",
        "test_dir = '/content/drive/MyDrive/Data/dataset_split3/test/'\n",
        "\n",
        "train_dataset = StrokeDataset(train_dir, transform=transform)\n",
        "val_dataset = StrokeDataset(val_dir, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "72a75245",
      "metadata": {
        "id": "72a75245"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "import torch.optim as optim\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "model.fc = nn.Linear(model.fc.in_features, 1)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "062c2089",
      "metadata": {
        "id": "062c2089"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "train_acc = []\n",
        "val_acc = []\n",
        "best_val_loss = float('inf')\n",
        "trigger_times = 0\n",
        "num_epochs = 100\n",
        "patience = 3\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images).squeeze(1)\n",
        "        loss = criterion(outputs, labels.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        predicted = torch.round(torch.sigmoid(outputs))\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "        total_train += labels.size(0)\n",
        "\n",
        "    train_losses.append(train_loss / len(train_loader))\n",
        "    train_acc.append(correct_train / total_train)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images).squeeze(1)\n",
        "            loss = criterion(outputs, labels.float())\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            predicted = torch.round(torch.sigmoid(outputs))\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "            total_val += labels.size(0)\n",
        "\n",
        "    val_losses.append(val_loss / len(val_loader))\n",
        "    val_acc.append(correct_val / total_val)\n",
        "\n",
        "    current_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    if current_val_loss < best_val_loss:\n",
        "        best_val_loss = current_val_loss\n",
        "        trigger_times = 0\n",
        "\n",
        "    else:\n",
        "        trigger_times += 1\n",
        "        print(f\"EarlyStop trigger # {trigger_times}\")\n",
        "\n",
        "        if trigger_times >= patience:\n",
        "            print(\"Early stopped...\")\n",
        "            break\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_losses[-1]:.4f} - Val Loss: {val_losses[-1]:.4f} - Train Accuracy: {train_acc[-1]:.4f} - Val Accuracy: {val_acc[-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77579ebc",
      "metadata": {
        "id": "77579ebc",
        "outputId": "b8d9ce61-d944-4e5e-cf91-124e672d2ede"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 97.63%\n"
          ]
        }
      ],
      "source": [
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images).squeeze()\n",
        "        predicted = (outputs > 0.5).float()\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "test_accuracy = (correct / total * 100)\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16a285ee",
      "metadata": {
        "id": "16a285ee"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_acc, label='Train Accuracy', marker='o')\n",
        "plt.plot(val_acc, label='Validation Accuracy', marker='s')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label=\"Training Loss\", marker=\"o\")\n",
        "plt.plot(val_losses, label=\"Validation Loss\", marker=\"s\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Training vs. Validation Loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c6d1e4f",
      "metadata": {
        "id": "2c6d1e4f"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "misclassified = []\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images).squeeze()\n",
        "        predicted = (outputs > 0.5).float()  # Convert probabilities to 0 or 1\n",
        "\n",
        "        # Store incorrect predictions\n",
        "        for i in range(len(images)):\n",
        "            if predicted[i] != labels[i]:  # Only store incorrect ones\n",
        "                if len(misclassified) < 20:  # Limit to 20 images\n",
        "                    img = images[i].cpu().numpy().transpose(1, 2, 0)\n",
        "                    img = (img * 0.5) + 0.5  # Undo normalization\n",
        "\n",
        "                    misclassified.append((img, predicted[i].item(), labels[i].item()))\n",
        "                else:\n",
        "                    break\n",
        "        if len(misclassified) >= 20:\n",
        "            break\n",
        "\n",
        "\n",
        "df = pd.DataFrame(misclassified, columns=[\"Image\", \"Predicted\", \"Actual\"])\n",
        "label_map = {0: \"Normal\", 1: \"Stroke\"}\n",
        "df[\"Predicted\"] = df[\"Predicted\"].map(label_map)\n",
        "df[\"Actual\"] = df[\"Actual\"].map(label_map)\n",
        "\n",
        "# --- Misclassified images ---\n",
        "fig, ax = plt.subplots(nrows=len(df), ncols=2, figsize=(6, len(df) * 2))\n",
        "\n",
        "for i, (image, pred, actual) in enumerate(zip(df[\"Image\"], df[\"Predicted\"], df[\"Actual\"])):\n",
        "    ax[i, 0].imshow(image, cmap=\"gray\")\n",
        "    ax[i, 1].text(0.5, 0.5, f\"Pred: {pred}\\nActual: {actual}\",\n",
        "                  fontsize=12, ha=\"center\", va=\"center\",\n",
        "                  bbox=dict(facecolor=\"white\", edgecolor=\"black\"))\n",
        "    ax[i, 1].axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59ec9339",
      "metadata": {
        "id": "59ec9339"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images).squeeze(1)\n",
        "        predicted = torch.round(torch.sigmoid(outputs))  # for binary classification\n",
        "\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "# Confusion matrix\n",
        "plt.figure(figsize=(5, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "748d94e0",
      "metadata": {
        "id": "748d94e0"
      },
      "outputs": [],
      "source": [
        "torch.save(model, \"resnet_model2.pth\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}